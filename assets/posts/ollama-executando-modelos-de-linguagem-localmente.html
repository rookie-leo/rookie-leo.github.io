<!DOCTYPE html>
<html lang="pt-BR">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Ollama ‚Äî Executando Modelos de Linguagem Localmente</title>
        <link rel="stylesheet" href="/assets/css/style.css">
        <link rel="stylesheet" href="/assets/css/posts.css">
    </head>

    <body>
        <header>
            <h1>Leonardo.DEV</h1>
            <nav>
                <a href="/index.html">Home</a>
                <a href="/about.html">Sobre</a>
            </nav>
        </header>

        <div class="post-container">

            <h1>Ollama: Executando Modelos de Linguagem Localmente</h1>
            <p class="post-date">Publicado em 25/11/2025</p>
            <p>Nos √∫ltimos anos, os modelos de linguagem (LLMs) se tornaram
                ferramentas indispens√°veis para desenvolvedores, pesquisadores e
                criadores de conte√∫do. No entanto, muitas dessas tecnologias
                dependem de processamento em nuvem, o que pode gerar limita√ß√µes
                relacionadas a custo, privacidade, lat√™ncia e conectividade.
                Nesse cen√°rio surge o Ollama, uma plataforma que facilita a
                execu√ß√£o de grandes modelos de linguagem localmente, diretamente
                no seu computador ‚Äî sem necessidade de internet e com foco em
                simplicidade, seguran√ßa e performance.
                Neste artigo, voc√™ vai aprender:
            </p>
            <div class="toc">
                <h3>üìö Sum√°rio</h3>
                <ul>
                    <li><a href="#oque-e-ollama">O que √© o Ollama?</a></li>
                    <li><a href="#como-funciona">Como funciona o
                            Ollama?</a></li>
                    <li><a href="#vantagens">Vantagens</a></li>
                    <li><a href="#limitacoes">Limita√ß√µes</a></li>
                    <li><a href="#instalacao">Instala√ß√£o</a></li>
                    <li><a href="#instalando-modelos">Instalando
                            Modelos</a></li>
                    <li><a href="#rodando-modelos">Rodando um Modelo</a></li>
                    <li><a href="#exemplos">Exemplos Pr√°ticos</a></li>
                    <li><a href="#conclusao">Conclus√£o</a></li>
                </ul>
            </div>

            <hr>

            <h2 id="oque-e-ollama">O que √© o Ollama?</h2>
            <p>
                O <strong>Ollama</strong> √© uma plataforma de c√≥digo aberto
                projetada para executar
                modelos de linguagem grandes diretamente no computador do
                usu√°rio. Ele permite gerar textos,
                auxiliar em programa√ß√£o e realizar tarefas de IA de forma
                privada e eficiente, sem depender da nuvem.
            </p>

            <div class="info-box">
                üí° <strong>Privacidade total:</strong> tudo roda localmente ‚Äî
                nenhum dado √© enviado para servidores externos.
            </div>

            <h2 id="como-funciona">Como funciona o Ollama?</h2>

            <p>
                O Ollama utiliza uma engine otimizada em Go e C++ para rodar
                modelos localmente usando CPU ou GPU,
                dependendo do hardware. Ele suporta modelos no formato
                <strong>GGUF</strong>, que s√£o vers√µes
                otimizadas dos grandes modelos de linguagem recentes.
            </p>

            <ul>
                <li>Llama 3</li>
                <li>Mistral</li>
                <li>Gemma</li>
                <li>Phi</li>
                <li>NeuralChat</li>
                <li>entre outros‚Ä¶</li>
            </ul>

            <p>Esses modelos s√£o quantizados, ou seja, reduzidos para ocupar
                menos mem√≥ria e permitir execu√ß√£o em m√°quinas comuns.</p>

            <h2 id="vantagens">Vantagens</h2>
            <ul>
                <li><strong>Privacidade</strong>: todos os dados ficam no seu
                    computador ‚Äî nada vai para a nuvem.</li>
                <li><strong>Zero custo recorrente</strong>: sem tokens, cr√©ditos
                    ou APIs pagas.</li>
                <li><strong>Instala√ß√£o simples</strong>: basta um comando.</li>
                <li><strong>√ìtima performance</strong> em GPUs compat√≠veis
                    (NVIDIA ou Apple Silicon).</li>
            </ul>

            <h2 id="limitacoes">Limita√ß√µes</h2>
            <ul>
                <li>Modelos grandes exigem bastante RAM ou GPU potente.</li>
                <li>Nem todos os recursos de IA da nuvem est√£o dispon√≠veis (como
                    RAG avan√ßado).</li>
                <li>Execu√ß√£o maior em CPU pode ser lenta.</li>
                <li>N√£o substitui servi√ßos corporativos como Bedrock ou OpenAI
                    API.</li>
            </ul>

            <h2 id="instalacao">Instala√ß√£o</h2>

            <ol>
                <li>Acesse: <a href="https://ollama.com"
                        target="_blank">https://ollama.com</a></li>
                <li>Baixe para:
                    <ul>
                        <li>Windows</li>
                        <li>macOS</li>
                        <li>Linux</li>
                    </ul>
                </li>
                <li>Abra o terminal e execute:
                    <pre><code>ollama</code></pre>
                </li>
                <li>Se o menu de ajuda aparecer, tudo est√° funcionando.</li>
            </ol>

            <div class="alert-box">
                ‚ö†Ô∏è Caso o comando <code>ollama</code> n√£o funcione no Windows,
                reinicie o terminal ou o computador ‚Äî o instalador adiciona o
                Ollama ao PATH.
            </div>

            <h2 id="instalando-modelos">Instalando Modelos</h2>

            <p>Baixe qualquer modelo com:</p>
            <pre><code>ollama pull &lt;modelo&gt;</code></pre>

            <p><strong>Exemplos:</strong></p>
            <ul>
                <li><code>ollama pull llama3.1</code></li>
                <li><code>ollama pull mistral</code></li>
                <li><code>ollama pull gemma:2b</code></li>
            </ul>

            <p>Para listar os modelos instalados:</p>
            <pre><code>ollama list</code></pre>

            <h2 id="rodando-modelos">Rodando um Modelo</h2>

            <pre><code>ollama run &lt;modelo&gt;</code></pre>

            <p><strong>Exemplos:</strong></p>
            <ul>
                <li><code>ollama run llama3.1</code></li>
                <li><code>ollama run mistral</code></li>
            </ul>

            <p>Quando aparecer o campo de envio de mensagem, voc√™ pode come√ßar a
                conversar com o modelo.</p>

            <h2 id="exemplos">Exemplos Pr√°ticos</h2>

            <ol>
                <li><strong>Resumir textos:</strong>
                    <pre><code>ollama run llama3.1 "Resuma o texto: A computa√ß√£o em nuvem permite..."</code></pre>
                </li>

                <li><strong>Gerar c√≥digo:</strong>
                    <pre><code>ollama run mistral "Crie uma fun√ß√£o Java que filtre uma lista de usu√°rios."</code></pre>
                </li>

                <li><strong>Explicar logs:</strong>
                    <pre><code>ollama run llama3.1 "Explique esse stacktrace: &lt;cole o stacktrace&gt;"</code></pre>
                </li>

                <li><strong>Criar conte√∫do:</strong>
                    <pre><code>ollama run llama3.1 "Escreva um t√≠tulo para artigo sobre arquitetura de microsservi√ßos."</code></pre>
                </li>
            </ol>

            <h2 id="conclusao">Conclus√£o</h2>

            <p>
                O Ollama permite executar modelos de linguagem de forma local,
                simples e privada.
                Agora voc√™ j√° sabe como instalar, rodar e testar seus primeiros
                modelos.
                A partir daqui, pode explorar o enorme potencial que essa
                ferramenta oferece.
            </p>

            <a class="back-link" href="/index.html">‚Üê Voltar</a>

        </div>
    </body>
</html>
